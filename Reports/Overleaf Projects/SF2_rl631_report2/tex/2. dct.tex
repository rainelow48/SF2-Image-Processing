The DCT is an energy compaction technique by replacing non-overlapping blocks of pixels with a blocks of the same number of transform coefficients, by transforming both the rows and columns using Equation \ref{eqn:dct}. Compression is achieved by quantisation of the transformed image. The original image can be recovered similarly, by applying the inverse transform on the blocks, Equation \ref{eqn:idct}. 
% Maybe delete
It is worth noting that the image is not affected by the order of transformation (row/column first) as the transformation is linear and separable.\\

The image \texttt{lighthouse.mat} is transformed in blocks of $8 \times 8$ and regrouped such that the coefficients of the same frequency are placed in the same sub-image, as shown on the left in Figure \ref{fig:DCT regrouped and basis function}. The energy of the sub-image is observed to decrease as frequency increases, since there are less details observed in images with higher frequencies, with the DC energy at $1.77 \times 10^6$ and the highest frequency sub-image energy at $483$. The DCT analyses each $8 \times 8$ block of image pixels into a linear combination of sixty-four $8 \times 8$ basis functions, as shown on the right in Figure \ref{fig:DCT regrouped and basis function}. The basis on the top left corresponds to the DC basis function, with increasing horizontal and vertical frequency towards the bottom right. The DCT coefficients are therefore the corresponding weights applied to each basis function to construct the original $8 \times 8$ image block.\\

The transformed image is quantised with a step size of 17, and subsequently regrouped and reconstructed, to give the left and right images respectively in Figure \ref{fig:DCT quantised}. The regrouped sub-images have different probability distributions as observed from the different range of pixel intensities within each sub-image. The smaller range of intensities for the high frequency sub-images results in a lower entropy of the sub-images. This results in less bits being encoded, hence an improved compression ratio, when encoding each sub-image separately instead of the quantised image as a whole. The total number of bits for encoding the quantised image as a whole and as separate sub-images are $1.10 \times 10^5$ and $9.75 \times 10^4$, giving a compression ratio of $2.08$ and $2.34$ respectively, with an improvement of $1.125$. The root-mean-squared (RMS) error of the reconstructed image is $3.76$, smaller than that of direct quantisation, at $4.86$. The reconstructed image is also observed to have block artefacts as correlation between adjacent blocks is not captured by the DCT.\\

The performance of a N-point DCT, where $N = 4, 8, 16$, is evaluated at quantisation step sizes adjusted to give an RMS error of approximately $4.86$. The respective step sizes are $23.900$, $23.691$ and $22.336$, with the total bits at $8.62 \times 10^4$, $7.75 \times 10^4$ and $7.91 \times 10^5$, giving compression ratios of $2.65$, $2.94$ and $2.88$ respectively. The corresponding reconstructed images are shown in Figure \ref{fig:DCT 4 8 16}. As $N$ increases, the block size gets larger and block artefacts are less prevalent in the reconstructed image, resulting in a smoother looking image. Considering the reconstructed image as well as the compression ratio, the optimal $N$ is therefore 8. With larger $N$, the image is split into smaller sub-images which likely has a smaller range of pixel intensities, thus smaller entropy. This results in a biased analysis. In the limiting case where $N=256$, each pixel is a sub-image and the entropy is therefore 0, resulting in 0 total bits and an infinite compression ratio, which is unrealistic. The compression ratios are recalculated for the N-point DCT encoded as $16 \times 16$ sub-images and found to be $3.20$, $3.21$ and $2.88$ respectively, which agrees with previous understanding.
